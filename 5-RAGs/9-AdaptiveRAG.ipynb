{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c304c8d1",
   "metadata": {},
   "source": [
    "#### AdaptiveRAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e70c656",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a45c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "\n",
    "\n",
    "embedding_model=OpenAIEmbeddings(model='text-embedding-3-small')\n",
    "llm=ChatOpenAI(model='gpt-4o-mini')\n",
    "\n",
    "urls=[\n",
    "    'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/',\n",
    "    'https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/',\n",
    "    'https://lilianweng.github.io/posts/2024-07-07-hallucination/'\n",
    "]\n",
    "\n",
    "\n",
    "docs=[WebBaseLoader(url).load() for url in urls ]\n",
    "docs_list=[item for sublist in docs for item in sublist]\n",
    "\n",
    "text_splitter=RecursiveCharacterTextSplitter.from_tiktoken_encoder(chunk_size=500,\n",
    "                                                                   chunk_overlap=0)\n",
    "\n",
    "docs_split=text_splitter.split_documents(docs_list)\n",
    "\n",
    "\n",
    "vector_store=FAISS.from_documents(\n",
    "    documents=docs_split,\n",
    "    embedding=embedding_model\n",
    ")\n",
    "\n",
    "retriever=vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8c74981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasources='web_search'\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasources.\"\"\"\n",
    "    datasources:Literal['vector_store','web_search']=Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose to route it to web search or a vectorstore.\"\n",
    "    )\n",
    "\n",
    "structured_llm_router=llm.with_structured_output(RouteQuery)\n",
    "\n",
    "system = \"\"\"\n",
    "You are an expert at routing a user question to a vectorstore or web search.\n",
    "- Route to 'vector_store' if the question is about:\n",
    "  - prompt engineering\n",
    "  - prompting techniques\n",
    "  - LLM hallucination\n",
    "  - adversarial attacks\n",
    "- Otherwise, route to 'web_search'.\n",
    "\"\"\"\n",
    "\n",
    "route_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        ('system',system),\n",
    "        ('human','{question}'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_router=route_prompt|structured_llm_router\n",
    "\n",
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\":\"For which club ronaldo plays?\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "32aaa0b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datasources='vector_store'\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    question_router.invoke(\n",
    "        {\"question\":\"Prompting techniques?\"}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c2b40493",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score:str=Field(\n",
    "        description=\"Document are relevant to the question,'yes' or 'no'\"\n",
    "        \n",
    "    )\n",
    "\n",
    "structured_llm_grader=llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "system=\"You are a grader assessing relevance of a retrieved document to a user question.\\n\" \\\n",
    "\"if the document contains keyword(s) or semantic meaning related to the user question,grade it as relevant.\\n\" \\\n",
    "\"it does not need to be stringent test.the goal is to filter out errorneous retrievals.\\n\" \\\n",
    "\"give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\n",
    "\n",
    "grade_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system),\n",
    "        (\"human\",\"Retrieved document:\\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader=grade_prompt|structured_llm_grader\n",
    "question=\"Prompting techniques\"\n",
    "docs=retriever.invoke(question)\n",
    "doc_text=docs[1].page_content\n",
    "print(retrieval_grader.invoke({'question':question,'document':doc_text}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e602a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompting techniques, also known as Prompt Engineering or In-Context Prompting, involve methods to effectively communicate with language models (LLMs) to guide their behavior without altering the model weights. Two basic approaches are zero-shot and few-shot learning; zero-shot relies on providing the task text directly, while few-shot gives high-quality demonstrations to improve understanding. Advanced techniques include Chain-of-Thought prompting and Self-Ask methods that utilize iterative questioning for deeper reasoning processes.\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "prompt=hub.pull('rlm/rag-prompt')\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain=prompt|llm|StrOutputParser()\n",
    "\n",
    "generation=rag_chain.invoke({'context':docs,'question':question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8615655",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeHallucinations(binary_score='yes')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeHallucinations(BaseModel):\n",
    "    \"\"\"Binary score for hallucination present in generation answer.\"\"\"\n",
    "    binary_score:str=Field(\n",
    "        description=\"Answer is grounded in the facts,'yes' or 'no' \"\n",
    "    )\n",
    "\n",
    "structured_llm_grader=llm.with_structured_output(GradeHallucinations)\n",
    "\n",
    "system=\"\"\"You are a grader assessing whether an LLM generation is grounded in / supported by a set of retrieved facts.\\n\n",
    "give a binary score 'yes' or 'no'. 'yes' means the answer is grounded in / supported by the set of facts.\"\"\"\n",
    "\n",
    "hallucination_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system),\n",
    "        (\"human\",\"set of facts: \\n\\n {documents} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "hallucination_grader=hallucination_prompt|structured_llm_grader\n",
    "hallucination_grader.invoke({'documents':docs,'generation':generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dc102f25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradeAnswer(binary_score='yes')"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class GradeAnswer(BaseModel):\n",
    "    \"\"\"Binary score to assess answer addresses question.\"\"\"\n",
    "\n",
    "    binary_score:str=Field(\n",
    "        description=\"Answer addresses the question, 'yes' or 'no' \"\n",
    "    )\n",
    "\n",
    "\n",
    "structured_llm_grader=llm.with_structured_output(GradeAnswer)\n",
    "\n",
    "system=\"\"\"You are a grader assessing whether an answer addresses/resolves a question \\n\n",
    "Give a binary score 'yes' or 'no'. 'Yes' means the answer resolves the question.\"\"\"\n",
    "\n",
    "answer_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system),\n",
    "        (\"human\",\"user question:\\n\\n {question} \\n\\n LLM generation: {generation}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "answer_grader=answer_prompt|structured_llm_grader\n",
    "answer_grader.invoke({'question':question,'generation':generation})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "47b4d8f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What are effective prompting techniques for enhancing communication and engagement?'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system=\"\"\"You are a re-writer that converts an input question to a better version that is optimized\n",
    "for vectorstore retrieval. Look at the input and try to reason about the underlying semantic intent/meaning.\"\"\"\n",
    "\n",
    "re_write_prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"here is the initial question:\\n\\n{question} \\n Formulate an improved question.\"\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter=re_write_prompt|llm|StrOutputParser()\n",
    "question_rewriter.invoke({'question':question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4d41caf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "web_search_tool=TavilySearchResults(k=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4827d96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    question:str\n",
    "    generation:str\n",
    "    documents:List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d00f1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "\n",
    "def retrieve(state):\n",
    "    print(\"--RETRIEVE--\")\n",
    "    question=state['question']\n",
    "\n",
    "    documents=retriever.invoke(question)\n",
    "    return {'documents':documents,'question':question}\n",
    "\n",
    "\n",
    "def generate(state):\n",
    "    print(\"--GENERATE--\")\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "\n",
    "    generation=rag_chain.invoke({'context':documents,'question':question})\n",
    "    return {'documents':documents,'question':question,'generation':generation}\n",
    "\n",
    "\n",
    "def grade_document(state):\n",
    "    print(\"--CHECK DOCUMENT RELEVANCE TO QUESTION--\")\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "\n",
    "\n",
    "    filtered_docs=[]\n",
    "    for document in documents:\n",
    "        score=retrieval_grader.invoke(\n",
    "            {'question':question,'document':document.page_content}\n",
    "        )\n",
    "        grade=score.binary_score\n",
    "        if grade=='yes':\n",
    "            print(\"--GRADE:DOCUMENT RELEVANT--\")\n",
    "            filtered_docs.append(documents)\n",
    "        else:\n",
    "            print(\"--GRADE:DOCUMENT NOT RELEVANT\")\n",
    "            continue\n",
    "    return {'documents':filtered_docs,'question':question}\n",
    "    \n",
    "    \n",
    "def transform_query(state):\n",
    "    print(\"--TRANSFORM QUERY--\")\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "\n",
    "    better_question=question_rewriter.invoke({'question':question})\n",
    "    return {'documents':documents,'question':better_question}\n",
    "\n",
    "\n",
    "def web_search(state):\n",
    "    print(\"--WEB SEARCH--\")\n",
    "    question=state['question']\n",
    "    docs=web_search_tool.invoke({'query':question})\n",
    "    web_results=\"\\n\".join([document['content'] for document in docs])\n",
    "    web_results=Document(page_content=web_results)\n",
    "\n",
    "    return {'documents':web_results,\"question\":question}\n",
    "\n",
    "\n",
    "def route_question(state):\n",
    "    question=state['question']\n",
    "    source=question_router.invoke({'question':question})\n",
    "    if source.datasources=='web_search':\n",
    "        print(\"--ROUTE QUESTION TO WEB-SEARCH--\")\n",
    "        return \"web_search\"\n",
    "    elif source.datasources==\"vector_store\":\n",
    "        print(\"--ROUTE QUESTION TO RAG--\")\n",
    "        return \"vector_store\"\n",
    "    return \"web_search\"\n",
    "    \n",
    "\n",
    "def decide_to_generate(state):\n",
    "    print(\"--ASSESS GRADED DOCUMENTS--\")\n",
    "    state['question']\n",
    "    filtered_douments=state['documents']\n",
    "\n",
    "    if not filtered_douments:\n",
    "        print(\"--DECISION:ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION,TRANSFORM QUERY--\")\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        print(\"--DECISION:GENERATE--\")\n",
    "        return \"generate\"\n",
    "    \n",
    "\n",
    "def grade_generation_v_document_and_question(state):\n",
    "    print(\"--CHECK HALLUCINATIONS--\")\n",
    "    question=state['question']\n",
    "    documents=state['documents']\n",
    "    generation=state['generation']\n",
    "\n",
    "    score=hallucination_grader.invoke(\n",
    "        {'documents':documents,'generation':generation}\n",
    "    )\n",
    "    grade=score.binary_score\n",
    "\n",
    "    if grade==\"yes\":\n",
    "        print(\"--DECISION: GENERATION IS GROUNDED IN DOCUMENTS--\")\n",
    "        print(\"--GRADE GENERATIONvsQUESTION--\")\n",
    "        score=answer_grader.invoke({'question':question,'generation':generation})\n",
    "        grade=score.binary_score\n",
    "        if grade==\"yes\":\n",
    "            print(\"--DECISION:GENERATION ADDRESSES QUESTION--\")\n",
    "            return \"useful\"\n",
    "        else:\n",
    "            print(\"--DECISION: GENERATION DOES NOT ADDRESS QUESTION--\")\n",
    "            return \"not useful\"\n",
    "    else:\n",
    "        print(\"--DECISION:GENERATION IS NOT GROUNDED IN DOCUMENTS,RE-TRY--\")\n",
    "        return \"not supported\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8054a81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x7f893f547800>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START,END,StateGraph\n",
    "\n",
    "workflow=StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"web_search\",web_search)\n",
    "workflow.add_node('retrieve',retrieve)\n",
    "workflow.add_node('grade_document',grade_document)\n",
    "workflow.add_node('generate',generate)\n",
    "workflow.add_node('transform_query',transform_query)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    START,\n",
    "    route_question,\n",
    "    {\n",
    "        \"web_search\":\"web_search\",\n",
    "        \"vector_store\":\"retrieve\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge('web_search','generate')\n",
    "workflow.add_edge('retrieve','grade_document')\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_document\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\":\"transform_query\",\n",
    "        \"generate\":\"generate\"\n",
    "    },\n",
    ")\n",
    "workflow.add_edge(\"transform_query\",\"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"generate\",\n",
    "    grade_generation_v_document_and_question,\n",
    "    {\n",
    "        \"not supported\":'generate',\n",
    "        'useful':END,\n",
    "        \"not useful\":\"transform_query\"\n",
    "    },\n",
    ")\n",
    "\n",
    "app=workflow.compile()\n",
    "\n",
    "app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4c8e2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ROUTE QUESTION TO WEB-SEARCH--\n",
      "--WEB SEARCH--\n",
      "--GENERATE--\n",
      "--CHECK HALLUCINATIONS--\n",
      "--DECISION: GENERATION IS GROUNDED IN DOCUMENTS--\n",
      "--GRADE GENERATIONvsQUESTION--\n",
      "--DECISION:GENERATION ADDRESSES QUESTION--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Who won the uefa champions league 2025?',\n",
       " 'generation': \"Paris Saint-Germain won the 2025 UEFA Champions League final, defeating Inter Milan 5-0. This victory marked PSG's first Champions League title in history. The match took place at the Allianz Arena in Munich on May 31, 2025.\",\n",
       " 'documents': Document(metadata={}, page_content='About Us\\n Careers\\n Privacy Preferences\\n Terms of Use\\n Privacy Policy\\n Cookie Policy\\n\\n# Who won the UEFA Champions League final 2025? How PSG made history with stunning victory over Inter Milan\\n\\nJoe Wright\\n\\n•\\n\\nParis Saint-Germain and Inter Milan locked horns in the final of the UEFA Champions League in 2025.\\n\\nThe Nerazzurri were in their second final in three seasons and hoping to do better than they did in 2023, when they lost 1-0 to Manchester City in Istanbul. [...] Treble-chasing PSG were hoping to win the trophy for the first time in their history and cap an impressive season under Luis Enrique.\\n\\nWhat transpired was one of the most dominant final performances we have ever seen.\\n\\nMORE: Teams and coaches to win the treble in European football\\n\\n## Who won the UEFA Champions League final 2025?\\n\\nParis Saint-Germain won the 2025 Champions League final after an astonishing 5-0 victory over Inter Milan in the final in Munich. [...] Inter had beaten Barcelona in an enthralling semifinal contest to earn a place in the final, and it was felt they were perhaps the best-placed team to stop PSG given their tactical flexibility and effectiveness on the break.\\n\\nBut Simone Inzaghi\\'s side simply had no answer at the Allianz Arena. PSG dominated from start to finish, with Inter only managing two shots on target in the 90 minutes.\\nThe match was played on 31 May 2025 at the Allianz Arena in Munich. The two teams playing were Paris Saint-Germain (PSG) of France, and Internazionale Milan of Italy. PSG won 5–0, a record final margin.\\n\\nIt was the first Champions League final since 2004 that did not include a team from England, Spain, or Germany. It was also the first final played using the new Swiss system\") format. [...] 4. ↑ \"2025 UEFA Champions League final: Munich Football Arena\". UEFA.com. Union of European Football Associations. 19 February 2025. Retrieved 20 February 2025.\\n5. ↑ \"What\\'s next for Bayern after Champions League play-off?\". FCBayern.com. FC Bayern Munich. 18 February 2025. Retrieved 20 February 2025.\\n6. ↑ \"Full Time Report Final – Paris Saint-Germain v Inter Milan\" (PDF). UEFA.com. Union of European Football Associations. 31 May 2025. Retrieved 31 May 2025. [...] | Paris Saint-Germain | 5–0 | Inter Milan |\\n --- \\n|  Hakimi  12\\'  Doué\")  20\\', 63\\'  Kvaratskhelia  73\\'  Mayulu  86\\' | Report |  |\\n\\nAllianz Arena, Munich\\n\\nAttendance: 64,327\\n\\nReferee: István Kovács (Romania)\\n\\n|  |  |\\n --- |\\n| Paris Saint-Germain | Inter Milan |\\n# 2025 UEFA Champions League Final: PSG defeats Inter Milan 5-0 to secure 1st UCL Trophy\\n## CBS Sports\\n1050000 subscribers\\n226 likes\\n\\n### Description\\n69139 views\\nPosted: 31 May 2025\\nMichael Lahoud joins CBS Sports HQ to recap the win for PSG over Inter Milan to win their first UEFA Champions League Final.\\n\\nSUBSCRIBE TO OUR CHANNEL:\\n\\nWATCH\\nCBS Sports HQ: \\nParamount Plus: \\n \\nFOLLOW US ON:\\nFacebook - \\nInstagram - \\nTwitter - \\n\\n#UEFA #championsleaguefinal #PSG #InterMilan [...] So, a lot to fix for Inter Milan as they try again next year. Michael Leah Hood helping break it down. We are still watching PSG celebrate it all as they pick up the largest win in Champions League history there. that 5nil victory over Inter Milan. They are your 2024 2025 Champions League title winners as you look at the other largest wins in a final. The last time we saw a deficit close to being this large was back in 1994 when AC Milan beat Barcelona 4-nil. [Music]\\n# Olympic gold medallist Enrique leads PSG to first UEFA Champions League title with demolition of Inter Milan in 2025 final\\n\\nFrench giants PSG finally lift the European Cup after one-sided men\\'s continental football final.\\n\\nPicture by 2025 Getty Images\\n\\nBy James Pratt\\n\\n31 May 2025 16:59 GMT-41 min read\\n\\nParis Saint-Germain have been crowned champions of Europe for the first time.\\n\\nPSG thrashed Internazionale Milan 5-0 in the 2025 UEFA men’s Champions League final on Saturday (31 May).\\n9. ^ \"Paris beat Monaco to lift Trophée des Champions\". Paris Saint-Germain. 5 January 2025. Retrieved 5 January 2025.\\n10. ^ Benge, James; Booth, Chuck (31 May 2025). \"Paris Saint-Germain vs. Inter score: PSG crowned Champions League winners for first time with blowout victory\". CBS Sports. Retrieved 31 May 2025.\\n11. ^ Jump up to: a b c d e f g h Stokkermans, Karel. \"European Champions\\' Cup/Champions League\". RSSSF. Retrieved 2 June 2025. [...] 28. ^ \"Höchster Finalsieg der Geschichte: PSG gewinnt erstmals die Champions League\" [Highest final victory in history: PSG wins the Champions League for the first time]. kicker \"Kicker (magazine)\") (in German). 31 May 2025. Retrieved 6 June 2025.\\n29. ^ Jump up to: a b Bonn, Kyle (31 May 2025). \"UEFA Champions League final score: PSG vs. Inter Milan result, highlights as Parisians win 2025 UCL title in record fashion\". The Sporting News. Retrieved 31 May 2025. [...] ### Details\\n\\n[edit]\\n\\nThe \"home\" team (for administrative purposes) was predetermined as the winner of semi-final 1 (Paris Saint-Germain).\\n\\n| Paris Saint-Germain | 5–0 | Inter Milan |\\n --- \\n|  Hakimi  12\\'  Doué  20\\', 63\\'  Kvaratskhelia  73\\'  Mayulu  86\\' | Report |  |\\n\\nAllianz Arena, Munich\\n\\nAttendance: 64,327\\n\\nReferee: István Kovács \"István Kovács (referee)\") (Romania)\\n\\n|  |  |\\n --- |\\n| Paris Saint-Germain | Inter Milan |')}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({'question':'Who won the uefa champions league 2025?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1d70565",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ROUTE QUESTION TO RAG--\n",
      "--RETRIEVE--\n",
      "--CHECK DOCUMENT RELEVANCE TO QUESTION--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--ASSESS GRADED DOCUMENTS--\n",
      "--DECISION:GENERATE--\n",
      "--GENERATE--\n",
      "--CHECK HALLUCINATIONS--\n",
      "--DECISION: GENERATION IS GROUNDED IN DOCUMENTS--\n",
      "--GRADE GENERATIONvsQUESTION--\n",
      "--DECISION:GENERATION ADDRESSES QUESTION--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Just list the Prompting Techniques?',\n",
       " 'generation': 'The prompting techniques include Basic Prompting (Zero-Shot and Few-Shot), Instruction Prompting (Self-Consistency Sampling and Chain-of-Thought), and Automatic Prompt Design. Additional methods like Self-Ask and Tree of Thoughts are also notable. Lastly, techniques involving external APIs and retrieval mechanisms can enhance prompting in specific contexts.',\n",
       " 'documents': [[Document(id='aaf8a8b9-bf8e-4afa-aec4-705adc1c03f5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"),\n",
       "   Document(id='f6fb546c-62f5-487a-9f62-5210414dd0ad', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       "   Document(id='8c6d5dbb-ea6d-4a8b-bdbf-189c02bbca4c', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Self-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\n\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.'),\n",
       "   Document(id='0369d18a-0bca-4935-8385-8b71a77d039d', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')],\n",
       "  [Document(id='aaf8a8b9-bf8e-4afa-aec4-705adc1c03f5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"),\n",
       "   Document(id='f6fb546c-62f5-487a-9f62-5210414dd0ad', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       "   Document(id='8c6d5dbb-ea6d-4a8b-bdbf-189c02bbca4c', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Self-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\n\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.'),\n",
       "   Document(id='0369d18a-0bca-4935-8385-8b71a77d039d', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')],\n",
       "  [Document(id='aaf8a8b9-bf8e-4afa-aec4-705adc1c03f5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"),\n",
       "   Document(id='f6fb546c-62f5-487a-9f62-5210414dd0ad', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       "   Document(id='8c6d5dbb-ea6d-4a8b-bdbf-189c02bbca4c', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Self-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\n\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.'),\n",
       "   Document(id='0369d18a-0bca-4935-8385-8b71a77d039d', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')],\n",
       "  [Document(id='aaf8a8b9-bf8e-4afa-aec4-705adc1c03f5', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Prompt Engineering\\n    \\nDate: March 15, 2023  |  Estimated Reading Time: 21 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nBasic Prompting\\n\\nZero-Shot\\n\\nFew-shot\\n\\nTips for Example Selection\\n\\nTips for Example Ordering\\n\\n\\n\\nInstruction Prompting\\n\\nSelf-Consistency Sampling\\n\\nChain-of-Thought (CoT)\\n\\nTypes of CoT prompts\\n\\nTips and Extensions\\n\\n\\nAutomatic Prompt Design\\n\\nAugmented Language Models\\n\\nRetrieval\\n\\nProgramming Language\\n\\nExternal APIs\\n\\n\\nCitation\\n\\nUseful Resources\\n\\nReferences\"),\n",
       "   Document(id='f6fb546c-62f5-487a-9f62-5210414dd0ad', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content=\"Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.\\n[My personal spicy take] In my opinion, some prompt engineering papers are not worthy 8 pages long, since those tricks can be explained in one or a few sentences and the rest is all about benchmarking. An easy-to-use and shared benchmark infrastructure should be more beneficial to the community. Iterative prompting or external tool use would not be trivial to set up. Also non-trivial to align the whole research community to adopt it.\\nBasic Prompting#\\nZero-shot and few-shot learning are two most basic approaches for prompting the model, pioneered by many LLM papers and commonly used for benchmarking LLM performance.\\nZero-Shot#\\nZero-shot learning is to simply feed the task text to the model and ask for results.\\n(All the sentiment analysis examples are from SST-2)\\nText: i'll bet the video game is a lot more fun than the film.\\nSentiment:\\nFew-shot#\\nFew-shot learning presents a set of high-quality demonstrations, each consisting of both input and desired output, on the target task. As the model first sees good examples, it can better understand human intention and criteria for what kinds of answers are wanted. Therefore, few-shot learning often leads to better performance than zero-shot. However, it comes at the cost of more token consumption and may hit the context length limit when input and output text are long.\\nText: (lawrence bounces) all over the stage, dancing, running, sweating, mopping his face and generally displaying the wacky talent that brought him fame in the first place.\\nSentiment: positive\"),\n",
       "   Document(id='8c6d5dbb-ea6d-4a8b-bdbf-189c02bbca4c', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Self-Ask (Press et al. 2022) is a method to repeatedly prompt the model to ask following-up questions to construct the thought process iteratively. Follow-up questions can be answered by search engine results. Similarly, IRCoT (Interleaving Retrieval CoT; Trivedi et al. 2022) and ReAct (Reason + Act; Yao et al. 2023) combines iterative CoT prompting with queries to Wikipedia APIs to search for relevant entities and content and then add it back into the context.\\n\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Press et al. 2022).\\n\\n\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, essentially creating a tree structure. The search process can be BFS or DFS while each state is evaluated by a classifier (via a prompt) or majority vote.\\n\\n\\n\\nHow Self-Ask works with external search queries.(Image source: Yao et al. 2022).\\n\\nAutomatic Prompt Design#\\nPrompt is a sequence of prefix tokens that increase the probability of getting  desired output given input. Therefore we can treat them as trainable parameters and optimize them directly on the embedding space via gradient descent, such as AutoPrompt (Shin et al., 2020, Prefix-Tuning (Li & Liang (2021)), P-tuning (Liu et al. 2021) and Prompt-Tuning (Lester et al. 2021). This section in my “Controllable Neural Text Generation” post has a good coverage of them. The trend from AutoPrompt to Prompt-Tuning is that the setup gets gradually simplified.\\nAPE (Automatic Prompt Engineer; Zhou et al. 2022) is a method to search over a pool of model-generated instruction candidates and then filters the candidate set according to a chosen score function to ultimately choose the best candidate with highest score.\\n\\n\\nPrompt LLM to generate instruction candidates based on a small set of demonstrations in the form of input-output pairs. E.g. {{Given desired input-output pairs}}\\\\n\\\\nThe instruction is.'),\n",
       "   Document(id='0369d18a-0bca-4935-8385-8b71a77d039d', metadata={'source': 'https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/', 'title': \"Prompt Engineering | Lil'Log\", 'description': 'Prompt Engineering, also known as In-Context Prompting, refers to methods for how to communicate with LLM to steer its behavior for desired outcomes without updating the model weights. It is an empirical science and the effect of prompt engineering methods can vary a lot among models, thus requiring heavy experimentation and heuristics.\\nThis post only focuses on prompt engineering for autoregressive language models, so nothing with Cloze tests, image generation or multimodality models. At its core, the goal of prompt engineering is about alignment and model steerability. Check my previous post on controllable text generation.', 'language': 'en'}, page_content='Weng, Lilian. (Mar 2023). Prompt Engineering. Lil’Log. https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/.\\n\\nOr\\n@article{weng2023prompt,\\n  title   = \"Prompt Engineering\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2023\",\\n  month   = \"Mar\",\\n  url     = \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\"\\n}\\nUseful Resources#\\n\\nOpenAI Cookbook has many in-depth examples for how to utilize LLM efficiently.\\nLangChain, a library for combining language models with other components to build applications.\\nPrompt Engineering Guide repo contains a pretty comprehensive collection of education materials on prompt engineering.\\nlearnprompting.org\\nPromptPerfect\\nSemantic Kernel')]]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({'question':'Just list the Prompting Techniques?'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "28665ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--ROUTE QUESTION TO RAG--\n",
      "--RETRIEVE--\n",
      "--CHECK DOCUMENT RELEVANCE TO QUESTION--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--GRADE:DOCUMENT RELEVANT--\n",
      "--ASSESS GRADED DOCUMENTS--\n",
      "--DECISION:GENERATE--\n",
      "--GENERATE--\n",
      "--CHECK HALLUCINATIONS--\n",
      "--DECISION: GENERATION IS GROUNDED IN DOCUMENTS--\n",
      "--GRADE GENERATIONvsQUESTION--\n",
      "--DECISION:GENERATION ADDRESSES QUESTION--\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Why LLM hallucinates?',\n",
       " 'generation': \"LLMs hallucinate due to the inherent issues in their vast pre-training datasets, which often contain outdated, missing, or incorrect information. During fine-tuning, introducing new knowledge can exacerbate the model's tendency to generate ungrounded or fabricated responses, especially when the model learns these examples slower than others. As a result, LLMs may provide answers that are inconsistent with factual information or external world knowledge.\",\n",
       " 'documents': [[Document(id='be6f2e3b-1bed-4e10-a4f8-156589a744f0', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.'),\n",
       "   Document(id='9e3b614a-abea-4d6d-bab2-d017177f3eef', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Weng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.'),\n",
       "   Document(id='aeb156d0-3b5a-4fe8-bbcf-ff92055ad29c', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content=\"Extrinsic Hallucinations in LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nWhat Causes Hallucinations?\\n\\nPre-training Data Issues\\n\\nFine-tuning New Knowledge\\n\\n\\nHallucination Detection\\n\\nRetrieval-Augmented Evaluation\\n\\nSampling-Based Detection\\n\\nCalibration of Unknown Knowledge\\n\\nIndirect Query\\n\\n\\nAnti-Hallucination Methods\\n\\nRAG → Edits and Attribution\\n\\nChain of Actions\\n\\nSampling Methods\\n\\nFine-tuning for Factuality\\n\\nFine-tuning for Attribution\\n\\n\\nAppendix: Evaluation Benchmarks\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\"),\n",
       "   Document(id='c0fc4c92-7bfc-4267-80f0-4f0ee52cf603', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Or\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).')],\n",
       "  [Document(id='be6f2e3b-1bed-4e10-a4f8-156589a744f0', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.'),\n",
       "   Document(id='9e3b614a-abea-4d6d-bab2-d017177f3eef', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Weng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.'),\n",
       "   Document(id='aeb156d0-3b5a-4fe8-bbcf-ff92055ad29c', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content=\"Extrinsic Hallucinations in LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nWhat Causes Hallucinations?\\n\\nPre-training Data Issues\\n\\nFine-tuning New Knowledge\\n\\n\\nHallucination Detection\\n\\nRetrieval-Augmented Evaluation\\n\\nSampling-Based Detection\\n\\nCalibration of Unknown Knowledge\\n\\nIndirect Query\\n\\n\\nAnti-Hallucination Methods\\n\\nRAG → Edits and Attribution\\n\\nChain of Actions\\n\\nSampling Methods\\n\\nFine-tuning for Factuality\\n\\nFine-tuning for Attribution\\n\\n\\nAppendix: Evaluation Benchmarks\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\"),\n",
       "   Document(id='c0fc4c92-7bfc-4267-80f0-4f0ee52cf603', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Or\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).')],\n",
       "  [Document(id='be6f2e3b-1bed-4e10-a4f8-156589a744f0', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.'),\n",
       "   Document(id='9e3b614a-abea-4d6d-bab2-d017177f3eef', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Weng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.'),\n",
       "   Document(id='aeb156d0-3b5a-4fe8-bbcf-ff92055ad29c', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content=\"Extrinsic Hallucinations in LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nWhat Causes Hallucinations?\\n\\nPre-training Data Issues\\n\\nFine-tuning New Knowledge\\n\\n\\nHallucination Detection\\n\\nRetrieval-Augmented Evaluation\\n\\nSampling-Based Detection\\n\\nCalibration of Unknown Knowledge\\n\\nIndirect Query\\n\\n\\nAnti-Hallucination Methods\\n\\nRAG → Edits and Attribution\\n\\nChain of Actions\\n\\nSampling Methods\\n\\nFine-tuning for Factuality\\n\\nFine-tuning for Attribution\\n\\n\\nAppendix: Evaluation Benchmarks\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\"),\n",
       "   Document(id='c0fc4c92-7bfc-4267-80f0-4f0ee52cf603', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Or\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).')],\n",
       "  [Document(id='be6f2e3b-1bed-4e10-a4f8-156589a744f0', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='This post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.\\nWhat Causes Hallucinations?#\\nGiven a standard deployable LLM goes through pre-training and fine-tuning for alignment and other improvements, let us consider causes at both stages.\\nPre-training Data Issues#\\nThe volume of the pre-training data corpus is enormous, as it is supposed to represent world knowledge in all available written forms. Data crawled from the public Internet is the most common choice and thus out-of-date, missing, or incorrect information is expected. As the model may incorrectly memorize this information by simply maximizing the log-likelihood, we would expect the model to make mistakes.\\nFine-tuning New Knowledge#\\nFine-tuning a pre-trained LLM via supervised fine-tuning and RLHF is a common technique for improving certain capabilities of the model like instruction following. Introducing new knowledge at the fine-tuning stage is hard to avoid.\\nFine-tuning usually consumes much less compute, making it debatable whether the model can reliably learn new knowledge via small-scale fine-tuning. Gekhman et al. 2024 studied the research question of whether fine-tuning LLMs on new knowledge encourages hallucinations. They found that (1) LLMs learn fine-tuning examples with new knowledge slower than other examples with knowledge consistent with the pre-existing knowledge of the model; (2) Once the examples with new knowledge are eventually learned, they increase the model’s tendency to hallucinate.\\nGiven a closed-book QA dataset (i.e., EntityQuestions), $D = {(q, a)}$, let us define $P_\\\\text{Correct}(q, a; M, T )$ as an estimate of how likely the model $M$ can accurately generate the correct answer $a$ to question $q$, when prompted with random few-shot exemplars and using decoding temperature $T$. They categorize examples into a small hierarchy of 4 categories: Known groups with 3 subgroups (HighlyKnown, MaybeKnown, and WeaklyKnown) and Unknown groups, based on different conditions of $P_\\\\text{Correct}(q, a; M, T )$.'),\n",
       "   Document(id='9e3b614a-abea-4d6d-bab2-d017177f3eef', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Weng, Lilian. (Jul 2024). Extrinsic Hallucinations in LLMs. Lil’Log. https://lilianweng.github.io/posts/2024-07-07-hallucination/.'),\n",
       "   Document(id='aeb156d0-3b5a-4fe8-bbcf-ff92055ad29c', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content=\"Extrinsic Hallucinations in LLMs | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n|\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      Extrinsic Hallucinations in LLMs\\n    \\nDate: July 7, 2024  |  Estimated Reading Time: 29 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nWhat Causes Hallucinations?\\n\\nPre-training Data Issues\\n\\nFine-tuning New Knowledge\\n\\n\\nHallucination Detection\\n\\nRetrieval-Augmented Evaluation\\n\\nSampling-Based Detection\\n\\nCalibration of Unknown Knowledge\\n\\nIndirect Query\\n\\n\\nAnti-Hallucination Methods\\n\\nRAG → Edits and Attribution\\n\\nChain of Actions\\n\\nSampling Methods\\n\\nFine-tuning for Factuality\\n\\nFine-tuning for Attribution\\n\\n\\nAppendix: Evaluation Benchmarks\\n\\nCitation\\n\\nReferences\\n\\n\\n\\n\\n\\nHallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\"),\n",
       "   Document(id='c0fc4c92-7bfc-4267-80f0-4f0ee52cf603', metadata={'source': 'https://lilianweng.github.io/posts/2024-07-07-hallucination/', 'title': \"Extrinsic Hallucinations in LLMs | Lil'Log\", 'description': 'Hallucination in large language models usually refers to the model generating unfaithful, fabricated, inconsistent, or nonsensical content. As a term, hallucination has been somewhat generalized to cases when the model makes mistakes. Here, I would like to narrow down the problem of hallucination to cases where the model output is fabricated and not grounded by either the provided context or world knowledge.\\nThere are two types of hallucination:\\n\\nIn-context hallucination: The model output should be consistent with the source content in context.\\nExtrinsic hallucination: The model output should be grounded by the pre-training dataset. However, given the size of the pre-training dataset, it is too expensive to retrieve and identify conflicts per generation. If we consider the pre-training data corpus as a proxy for world knowledge, we essentially try to ensure the model output is factual and verifiable by external world knowledge. Equally importantly, when the model does not know about a fact, it should say so.\\n\\nThis post focuses on extrinsic hallucination. To avoid hallucination, LLMs need to be (1) factual and (2) acknowledge not knowing the answer when applicable.', 'language': 'en'}, page_content='Or\\n@article{weng2024hallucination,\\n  title   = \"Extrinsic Hallucinations in LLMs.\",\\n  author  = \"Weng, Lilian\",\\n  journal = \"lilianweng.github.io\",\\n  year    = \"2024\",\\n  month   = \"Jul\",\\n  url     = \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\"\\n}\\nReferences#\\n[1] Ji et al. “Survey of hallucination in natural language generation.” ACM Computing Surveys (2022)\\n[2] Gekhman et al. “Does Fine-Tuning LLMs on New Knowledge Encourage Hallucinations?” arXiv preprint arXiv:2405.05904 (2024).\\n[3] Min et al. “FActScore: Fine-grained atomic evaluation of factual precision in long form text generation.” EMNLP 2023.\\n[4] Wei et al. 2024 “Long-form Factuality in LLMs” arXiv preprint arXiv:2403.18802 (2024).\\n[5] Chern et al. “FacTool: Factuality detection in generative AI - a tool augmented framework for multi-task and multi-domain scenarios.” arXiv preprint arXiv:2307.13528 (2023).\\n[6] Lin et al. “TruthfulQA: Measuring How Models Mimic Human Falsehoods.” ACL 2022.\\n[7] Yin et al. “Do Large Language Models Know What They Don’t Know?” ACL 2023.\\n[8] Kadavath et al. “Language Models (Mostly) Know What They Know” arXiv preprint arXiv:2207.05221 (2022).\\n[9] Agrawal et al. “Do language models know when they’re hallucinating references?” arXiv preprint arXiv:2305.18248 (2023).')]]}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app.invoke({'question':'Why LLM hallucinates?'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agenticai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
